# ðŸ—ï¸ MetricCare Terraform Infrastructure

This directory provisions the **AWS Data Lakehouse infrastructure** for the MetricCare project using **Terraform**.  
It automates the creation of compute, storage, and orchestration layers used across:
- `lambda/` â†’ Event-driven Glue triggers  
- `glue_job/` â†’ ETL transformations (Bronze â†’ Silver â†’ Gold)  
- `fhir_data/` â†’ Synthetic FHIR datasets used for CMS metrics  

---

## ðŸ“¦ Overview

| Component | AWS Service | Purpose |
|------------|--------------|----------|
| ðŸª£ **S3 Buckets** | Amazon S3 | Stores raw â†’ bronze â†’ silver â†’ gold FHIR datasets |
| âš™ï¸ **Glue Jobs & Workflow** | AWS Glue | Runs ETL pipelines and manages job dependencies |
| ðŸ§  **DynamoDB Table** | Amazon DynamoDB | Tracks processed files for incremental ingestion |
| ðŸ§© **Lambda Function** | AWS Lambda | Triggers Glue Workflow on new S3 data arrival |
| ðŸ”” **SNS + EventBridge** | AWS SNS / EventBridge | Notification and orchestration for job events |
| ðŸ” **IAM Roles & Policies** | AWS IAM | Grants least-privilege access to each service |

---

## ðŸ—‚ï¸ Terraform Project Structure

```bash
terraform/
â”‚
â”œâ”€â”€ backend.tf                # Defines remote backend (S3 + DynamoDB) for Terraform state
â”œâ”€â”€ main.tf                   # Root module calling all submodules (S3, Glue, Lambda, etc.)
â”œâ”€â”€ provider.tf               # AWS provider configuration and version locking
â”œâ”€â”€ variables.tf              # Shared variable definitions for all modules
â”œâ”€â”€ terraform.tfstate         # Local state file (only if remote backend not configured)
â”œâ”€â”€ terraform.tfstate.backup  # Local backup of the state file
â”œâ”€â”€ README.md                 # This documentation file
â”‚
â”œâ”€â”€ awscredentials/
â”‚   â””â”€â”€ credentials            # Stores AWS CLI credentials (for local dev use only)
â”‚
â””â”€â”€ module/
    â”œâ”€â”€ aws_sns/               # Creates SNS topics/subscriptions for notifications
    â”œâ”€â”€ dynemoDB/              # Creates DynamoDB table for processed_files metadata
    â”œâ”€â”€ event_bridge/          # Defines EventBridge rules for workflow scheduling
    â”œâ”€â”€ glue/
    â”‚   â”œâ”€â”€ glue_catalog_database/   # Creates Glue databases for bronze/silver/gold tables
    â”‚   â”œâ”€â”€ glue_job/                # Deploys ETL Glue Jobs (PySpark scripts)
    â”‚   â””â”€â”€ glue_workflow/           # Creates Glue Workflows + triggers between jobs
    â”œâ”€â”€ lambda_function/      # Deploys Lambda for workflow orchestration
    â””â”€â”€ s3/
        â”œâ”€â”€ s3_bucket/        # Creates data lake buckets (raw â†’ bronze â†’ silver â†’ gold)
        â”œâ”€â”€ upload_files/     # Uploads config/data files to S3 during provisioning
        â””â”€â”€ upload_scripts/   # Uploads Lambda & Glue scripts to corresponding S3 paths
```

---

## ðŸš€ Deploying with Terraform Workspaces

This project uses **Terraform Workspaces** to manage environments (e.g., `dev`, `prod`).

```bash
terraform init
terraform workspace new dev
terraform workspace select dev
terraform plan
terraform apply -auto-approve
```

> ðŸ§© Each workspace maintains isolated state, allowing you to deploy multiple environments using the same Terraform codebase.

---

## ðŸ§© Running or Destroying Specific Modules

You can apply or destroy **individual modules** instead of the entire stack.

### â–¶ï¸ Apply a Specific Module
```bash
terraform apply -target=module.lambda_function -auto-approve
```

### ðŸ’£ Destroy a Specific Module
```bash
terraform destroy -target=module.lambda_function -auto-approve
```

> âš ï¸ Use targeted commands for testing only. For production, always run full `plan/apply` for dependency consistency.

---

## ðŸ’¥ Destroying the Entire Infrastructure

To remove **all resources** in the current workspace:

```bash
terraform destroy -auto-approve
```

If managing multiple environments:

```bash
terraform workspace select dev
terraform destroy -auto-approve
```

> ðŸ§¹ This ensures a clean teardown of all resources in that workspace while preserving remote state in S3/DynamoDB.

---

## ðŸ§  Design Principles

- **Modular Architecture:** Each AWS service lives in an independent module.  
- **Workspace Isolation:** Environments (`dev`, `prod`) use workspaces instead of `.tfvars`.  
- **Least-Privilege IAM:** Policies grant only necessary permissions.  
- **Version Locking:** AWS provider pinned to `~> 6.12.0`.  
- **Automation-Ready:** EventBridge + Lambda enable serverless orchestration.  
- **State Safety:** Remote backend protects against conflicting deployments.  

---

## ðŸ”— Additional Resources

### ðŸ§° Terraform
- [Terraform Downloads](https://developer.hashicorp.com/terraform/downloads)  
- [Terraform CLI Documentation](https://developer.hashicorp.com/terraform/cli)  
- [Terraform AWS Provider Reference](https://registry.terraform.io/providers/hashicorp/aws/latest/docs)  
- [Terraform Remote State (S3 Backend)](https://developer.hashicorp.com/terraform/language/settings/backends/s3)  

### â˜ï¸ AWS CLI & Configuration
- [AWS CLI Installation Guide](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html)  
- [AWS CLI Configuration Files](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html)  
- [AWS IAM User Guide](https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html)  

### ðŸ§© AWS Service Docs
- [AWS Glue Documentation](https://docs.aws.amazon.com/glue/)  
- [AWS Lambda Documentation](https://docs.aws.amazon.com/lambda/)  
- [Amazon EventBridge Documentation](https://docs.aws.amazon.com/eventbridge/)  
- [AWS SNS Documentation](https://docs.aws.amazon.com/sns/)  
- [Amazon DynamoDB Documentation](https://docs.aws.amazon.com/dynamodb/)  
- [Amazon S3 Documentation](https://docs.aws.amazon.com/s3/)

---

> ðŸ§± **Author:** Shreyash (Data Engineer)  
> ðŸ“š *MetricCare â€“ AWS Data Lakehouse for Healthcare Analytics*  
> ðŸ”— *Built with Terraform, AWS Glue, Lambda, S3, DynamoDB, EventBridge, and SNS*
