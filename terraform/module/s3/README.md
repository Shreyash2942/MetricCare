# ðŸª£ AWS S3 Modules (MetricCare Lakehouse)

This directory contains all **Terraform modules for Amazon S3** used in the **MetricCare Data Lakehouse**.  
S3 serves as the **foundation of the data lake**, providing storage zones for raw FHIR data ingestion, processed Hudi tables, curated datasets, and analytics-ready metrics.

---

## ðŸ“– Overview

The MetricCare S3 architecture defines multiple buckets and prefixes to maintain a clear data lineage:

| Layer | Description |
|--------|--------------|
| **Raw** | Ingests raw FHIR JSON data generated by synthetic simulators. |
| **Bronze** | Stores transformed Parquet (Hudi) data from Glue jobs. |
| **Silver** | Holds cleaned and standardized datasets ready for analysis. |
| **Gold** | Contains aggregated CMS metric tables for dashboards (Mortality, Infection, Readmission, ALOS). |
| **Scripts / Configs** | Stores Glue & Lambda scripts and configuration files for automation. |

---

## ðŸ—‚ï¸ Folder Structure

```bash
s3/
â”œâ”€â”€ s3_bucket/        # Creates S3 buckets for each data zone (raw, bronze, silver, gold)
â”œâ”€â”€ upload_scripts/   # Uploads Lambda and Glue ETL scripts to S3
â”œâ”€â”€ upload_files/     # Uploads configuration and metadata files (e.g., config.json)
â”‚
â”œâ”€â”€ main.tf           # (Optional) Wrapper to invoke submodules together
â”œâ”€â”€ variables.tf      # (Optional) Common variables (e.g., project tags)
â”œâ”€â”€ outputs.tf        # (Optional) Combined outputs for integration with Glue or Lambda
â””â”€â”€ README.md         # This documentation file
```

---

## âš™ï¸ Submodules Summary

| Submodule | Purpose |
|------------|----------|
| [`s3_bucket/`](./s3_bucket) | Defines and provisions S3 buckets for each data layer. |
| [`upload_scripts/`](./upload_scripts) | Uploads Lambda and Glue ETL scripts to S3. |
| [`upload_files/`](./upload_files) | Uploads configuration, metadata, or sample input files to S3. |

---

## ðŸ” Architecture Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       AWS S3 Layer                         â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚  Raw â†’ Bronze â†’ Silver â†’ Gold â†’ Scripts â†’ Configs           â”‚
â”‚  (FHIR Data Flow)   (ETL Scripts)   (Metadata / JSON)       â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚  Glue & Lambda access all layers for ETL & orchestration.  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

- **Raw â†’ Bronze â†’ Silver â†’ Gold** forms the ETL pipeline path.  
- **upload_scripts/** keeps Glue and Lambda scripts versioned and accessible.  
- **upload_files/** ensures configuration files are consistent across environments.  

---

## ðŸ§© Example Combined Deployment

Hereâ€™s how you can deploy all S3 modules together:

```hcl
module "s3_bucket" {
  source = "./module/s3/s3_bucket"
  buckets = {
    raw     = "metriccare-raw-data"
    bronze  = "metriccare-bronze-data"
    silver  = "metriccare-silver-data"
    gold    = "metriccare-gold-data"
    scripts = "metriccare-upload-scripts"
  }
  enable_versioning = true
  enable_encryption = true
}

module "upload_scripts" {
  source = "./module/s3/upload_scripts"
  bucket_name = module.s3_bucket.bucket_names["scripts"]
  upload_file = {
    lambda_trigger = {
      s3_location     = "lambda/myglueworkflow.zip"
      source_location = "../lambda/myglueworkflow.zip"
    }
  }
}

module "upload_files" {
  source = "./module/s3/upload_files"
  bucket_name = module.s3_bucket.bucket_names["scripts"]
  upload_file = {
    config = {
      s3_location     = "configs/config.json"
      source_location = "../config/config.json"
    }
  }
}
```

---

## ðŸ“¤ Outputs and Integrations

| Integration | Description |
|--------------|--------------|
| **Glue Jobs** | Read/write datasets from Bronze â†’ Silver â†’ Gold zones. |
| **Lambda Functions** | Trigger Glue workflows when new S3 data arrives. |
| **EventBridge** | Automates ETL schedules using S3 event triggers. |
| **Athena / Power BI** | Query and visualize Gold-layer tables. |

---

## ðŸ§  Best Practices

- Use **consistent naming conventions** (e.g., `metriccare-raw-data`, `metriccare-gold-data`).  
- Enable **versioning and server-side encryption** for all buckets.  
- Tag buckets and objects with environment and project identifiers.  
- Keep **scripts/configs** in a single shared bucket for easier management.  
- Avoid public access; rely on **IAM-based permissions**.  
- Enable **S3 access logs** for auditing and troubleshooting.  

---

## ðŸ”— References

- [Terraform AWS S3 Bucket](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/s3_bucket)  
- [Terraform AWS S3 Object](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/s3_object)  
- [AWS S3 Best Practices](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-best-practices.html)  
- [AWS Glue Documentation](https://docs.aws.amazon.com/glue/latest/dg/what-is-glue.html)  
- [AWS Lambda Documentation](https://docs.aws.amazon.com/lambda/latest/dg/welcome.html)

---

> ðŸ§± **Author:** Shreyash (Data Engineer)  
> ðŸ“š *MetricCare â€“ AWS Data Lakehouse for Healthcare Analytics*  
> ðŸ”— *Module: s3 â€“ Data Storage, Script Management, and Config Automation*

