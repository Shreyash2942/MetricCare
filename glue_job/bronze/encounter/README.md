# üè• Bronze Layer ‚Äì Encounter FHIR Data Ingestion (AWS Glue + Hudi)

This script is part of the **MetricCare Data Lakehouse Pipeline**, responsible for ingesting **FHIR-compliant Encounter data** into the **Bronze Layer** using **AWS Glue**, **Apache Hudi**, and **Amazon DynamoDB** for incremental tracking.

---

## üß© Overview

This job processes **FHIR Encounter JSON files** stored in Amazon S3 and performs the following actions:

1. **Reads Encounter JSON data** generated by the FHIR Data Generator.
2. **Flattens FHIR structure** ‚Äî extracts important fields like patient reference, encounter type, and department.
3. **Adds metadata** columns:
   - `sourcename` ‚Üí input file path
   - `precombine_ts` ‚Üí ingestion timestamp (used for Hudi deduplication)
4. **Writes data to an Apache Hudi table** in the Bronze Zone.
5. **Logs ingestion statistics** to a Hudi log table.
6. **Updates DynamoDB metadata table** to track processed files and prevent re-ingestion.

---

## ‚öôÔ∏è Architecture Flow

```
          +--------------------------+
          | FHIR JSON (Encounter)    |
          | Stored in S3 Input Path  |
          +------------+-------------+
                       |
                       v
         +-------------+-------------+
         | AWS Glue ETL (Bronze Job) |
         |  - Parse Encounter JSON   |
         |  - Flatten Nested Fields  |
         |  - Add Metadata Columns   |
         +-------------+-------------+
                       |
                       v
          +------------+------------+
          | Apache Hudi Bronze Table|
          | Partitioned by Hospital |
          +------------+------------+
                       |
             +---------+---------+
             | AWS Glue Catalog |
             | Athena Queryable |
             +------------------+
                       |
          +------------+------------+
          | DynamoDB Meta Table     |
          | Tracks processed files  |
          +--------------------------+
```

---

## üìÇ Key Components

| Component | Description |
|------------|-------------|
| **AWS Glue Job** | Extracts and flattens Encounter JSON data |
| **Apache Hudi** | Provides incremental storage with upserts |
| **AWS Glue Catalog** | Registers schema for Athena queries |
| **Amazon DynamoDB** | Tracks processed files to enable incremental ingestion |

---

## ü©∫ Extracted Schema

| Column | Description |
|---------|-------------|
| `encounter_id` | Unique ID of the encounter |
| `status` | Current encounter lifecycle status |
| `encounter_type` | Encounter classification (Inpatient, Outpatient, etc.) |
| `patient_ref` | FHIR reference to the Patient resource |
| `admission_time` | Start datetime of the encounter |
| `discharge_time` | End datetime of the encounter |
| `department` | Department or location display name |
| `participant_role` | Role of the primary healthcare participant |
| `practitioner_ref` | Reference to the Practitioner resource |
| `hospital_name` | Hospital display name (from FHIR `serviceProvider.display`) |
| `sourcename` | Input file path from S3 |
| `precombine_ts` | Ingestion timestamp used by Hudi |

---

## üöÄ Hudi Configuration Summary

| Parameter | Description |
|------------|-------------|
| `hoodie.table.name` | Name of the Hudi table (bronze_encounter) |
| `hoodie.datasource.write.recordkey.field` | Primary key: `encounter_id` |
| `hoodie.datasource.write.precombine.field` | Deduplication key: `precombine_ts` |
| `hoodie.datasource.write.partitionpath.field` | Partition field: `hospital_name` |
| `hoodie.datasource.write.operation` | Mode: `upsert` |
| `hoodie.datasource.hive_sync.use_glue_catalog` | Sync schema to AWS Glue Catalog |

---

## üß† Incremental Ingestion Logic

Each run of the Glue job executes the following sequence:

1. **Fetch all JSON files** from S3 input prefix.
2. **Retrieve processed filenames** from DynamoDB.
3. **Process only new files** that aren‚Äôt yet in DynamoDB.
4. **Write transformed data** into the Bronze Hudi table.
5. **Update DynamoDB** with filenames of successfully processed files.

This ensures **idempotent processing** and safe re-runs.

---

## üß∞ Dependencies

Install the required libraries before testing locally or deploying:

```bash
pip install boto3 pyspark awsglue
```

---

## üîó Useful AWS References

### üîπ Apache Hudi
- [Hudi Overview](https://hudi.apache.org/docs/overview/)
- [Hudi on AWS Glue](https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-hudi.html)

### üîπ AWS Glue
- [AWS Glue ETL Docs](https://docs.aws.amazon.com/glue/latest/dg/what-is-glue.html)
- [AWS Glue with Hudi Catalog Sync](https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-hudi.html#aws-glue-programming-etl-hudi-catalog)

### üîπ DynamoDB
- [DynamoDB Overview](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html)
- [Using DynamoDB with Boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/dynamodb.html)

### üîπ AWS Lakehouse
- [Lakehouse Architecture on AWS](https://aws.amazon.com/solutions/guidance/lakehouse-architecture-on-aws/)

---

## ‚úÖ Summary

This **Bronze Encounter Glue Job** transforms raw FHIR Encounter JSON files into structured Hudi tables.  
It serves as the **foundation for Silver transformations**, supporting downstream analytics such as:

- Encounter durations and readmission tracking  
- Physician or department performance metrics  
- Hospital utilization analytics  

---

**Author:** MetricCare Data Engineering Team  
üìÖ Last Updated: October 2025  
