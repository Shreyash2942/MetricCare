# ü©∫ Bronze Layer ‚Äì Patient FHIR Data Ingestion (AWS Glue + Hudi)

This script is part of the **MetricCare Data Lakehouse Pipeline**, responsible for ingesting **FHIR-compliant Patient data** into the **Bronze Layer** on AWS using **AWS Glue**, **Apache Hudi**, and **DynamoDB** for incremental metadata tracking.

---

## üß© Overview

This Glue ETL job performs the following steps:

1. **Reads synthetic FHIR Patient JSON files** generated by the MetricCare FHIR Data Generator from S3.
2. **Flattens FHIR structure** (extracts key patient-level attributes such as `gender`, `birthDate`, `language`, `hospital_name`).
3. **Handles deceased logic** (adds `is_deceased` flag and handles missing `deceasedDateTime` safely).
4. **Adds metadata columns** for ingestion tracking:
   - `sourcename` ‚Üí original file path
   - `precombine_ts` ‚Üí ingestion timestamp
5. **Writes data into an Apache Hudi table** in the Bronze zone with Glue Catalog sync enabled.
6. **Logs ingestion activity** (table name, timestamp, column count) into a Hudi log table.
7. **Updates DynamoDB** with processed file names to support incremental ingestion (avoids re-processing).

---

## ‚öôÔ∏è Architecture Diagram

```
          +----------------------+
          | FHIR JSON (Patient)  |
          |     in S3 Input      |
          +----------+-----------+
                     |
                     v
        +------------+-------------+
        | AWS Glue ETL (Bronze Job)|
        | - Parse / Flatten JSON   |
        | - Add metadata fields    |
        | - Mark deceased patients |
        +------------+-------------+
                     |
                     v
          +----------+----------+
          |  Apache Hudi Table  |
          |  (Bronze Layer, COW)|
          +----------+----------+
                     |
        +------------+-------------+
        | AWS Glue Catalog + Athena|
        | for schema discovery     |
        +------------+-------------+
                     |
          +----------+----------+
          | DynamoDB Meta Table |
          | Tracks processed    |
          | filenames (CDC)     |
          +----------------------+
```

---

## üìÇ Key Components

| Component | Description |
|------------|--------------|
| **AWS Glue Job** | Executes the ETL script with dynamic configs from S3 |
| **Apache Hudi** | Provides incremental data lake storage (COPY_ON_WRITE) |
| **AWS Glue Catalog** | Manages schema and enables Athena queries |
| **Amazon S3** | Raw and processed data storage |
| **DynamoDB Meta Table** | Tracks processed file names to ensure idempotent runs |

---

## üìú Output Schema

| Column | Description |
|---------|--------------|
| `patient_id` | Unique FHIR UUID of the patient |
| `gender` | Gender of the patient |
| `birthDate` | Date of birth (yyyy-MM-dd) |
| `language` | Preferred language of communication |
| `hospital_name` | Managing organization name (from FHIR) |
| `is_deceased` | Boolean indicating death status |
| `deceasedDateTime` | Death timestamp (nullable) |
| `sourcename` | File path for traceability |
| `precombine_ts` | Ingestion timestamp used by Hudi |

---

## üöÄ Hudi Write Options Summary

| Parameter | Description |
|------------|-------------|
| `hoodie.table.name` | Table name (from config) |
| `hoodie.datasource.write.operation` | Set to `upsert` for incremental writes |
| `hoodie.datasource.write.recordkey.field` | Primary key (`patient_id`) |
| `hoodie.datasource.write.precombine.field` | Combines duplicates using `precombine_ts` |
| `hoodie.datasource.write.partitionpath.field` | Partitioned by `hospital_name` |
| `hoodie.datasource.hive_sync.use_glue_catalog` | Enables Glue Catalog sync |
| `hoodie.datasource.hive_sync.mode` | Set to `hms` for Glue-based sync |

---

## üß† Incremental Load Logic

Each job run:
1. Lists JSON files from S3 input prefix.
2. Queries DynamoDB for previously processed filenames.
3. Processes **only new files**.
4. Updates DynamoDB with filenames of successfully written files.

This ensures safe, **idempotent processing** (no re-ingestion of already processed data).

---

## üß∞ Dependencies

Install the required Python packages before running locally or testing:

```bash
pip install boto3 pyspark awsglue
```

---

## üîó Useful Links

### üîπ Apache Hudi
- [Hudi Official Docs](https://hudi.apache.org/docs/overview/)
- [Hudi on AWS Glue](https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-hudi.html)

### üîπ AWS Glue
- [AWS Glue ETL Overview](https://docs.aws.amazon.com/glue/latest/dg/what-is-glue.html)
- [Glue Catalog Integration with Hudi](https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-hudi.html#aws-glue-programming-etl-hudi-catalog)

### üîπ Amazon DynamoDB
- [DynamoDB Table Concepts](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.CoreComponents.html)
- [Using Boto3 with DynamoDB](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/dynamodb.html)

### üîπ AWS Data Lakehouse
- [AWS Lakehouse Architecture Overview](https://aws.amazon.com/solutions/guidance/lakehouse-architecture-on-aws/)

---

## ‚úÖ Summary

This **Bronze-level Glue Job** standardizes raw FHIR `Patient` data and prepares it for downstream Silver and Gold transformations.  
It provides the foundational layer for healthcare analytics such as:

- Mortality metrics  
- Readmission tracking  
- Population health insights  

---

**Author:** MetricCare Data Engineering Team  
üìÖ Last Updated: October 2025  
